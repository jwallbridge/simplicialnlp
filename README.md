# Simplicial Transformer Architecture   

This is the code repository for a 2- and 3-Simplicial Transformer Architecture.   It includes both an Encoder and Decoder.

Currently, the principal files are :
* `attention1_layer.py` a 1-simplicial attention layer.
* `attention2_layer.py` a 2-simplicial attention layer.
* `attention3_layer.py` a 3-simplicial attention layer.
* `transformer2_100.py` a Transformer with 2-simplicial Encoder and standard Decoder.

More will be added in due course.


# Referencing

This code was developed for the project [2simplicialtransformer](https://github.com/dmurfet/2simplicialtransformer) by James Clift, Dmitry Doryn, Daniel Murfet and James Wallbridge. If you find this helpful in your work, please consider citing the following :

```
@inproceedings{clift2020,    
  author = {James Clift and Dmitry Doryn and Daniel Murfet and James Wallbridge},    
  title = {Logic and the 2-Simplicial Transformer},    
  booktitle = {Proceedings of the International Conference on Learning Representations},    
  year = {2020},    
}
```
